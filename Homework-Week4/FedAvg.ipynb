{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('./utils/')\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to experiment with a different seed value, change 'trial_times'\n",
    "trial_times = 1\n",
    "SEED = 42 + trial_times -1\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "## If you want to change the hyperparameters, please change them here\n",
    "\n",
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20 \n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 300\n",
    "    self.local_epochs = 2\n",
    "    self.lr = 10**(-3)\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 300\n",
    "    self.worker_num = 20\n",
    "    self.participation_rate = 1\n",
    "    self.sample_num = int(self.worker_num * self.participation_rate)\n",
    "    self.total_data_rate = 1\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    ## If you use MNIST or CIFAR-10, the degree of data heterogeneity can be changed by changing alpha_label and alpha_size.\n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "    \n",
    "    ## Select a dataset from 'FEMNIST','Shakespeare','Sent140','MNIST', or 'CIFAR-10'.\n",
    "    self.dataset_name = 'FEMNIST'\n",
    "\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name=='FEMNIST':\n",
    "    from femnist_dataset import *\n",
    "    args.num_classes = 62\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='Shakespeare':\n",
    "    from shakespeare_dataset import *\n",
    "    model_name = \"RNN()\"\n",
    "    \n",
    "elif args.dataset_name=='Sent140':\n",
    "    from sent140_dataset import *\n",
    "    from utils_sent140 import *\n",
    "    VOCAB_DIR = '../models/embs.json'\n",
    "    _, indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
    "    model_name = \"RNNSent(args,'LSTM', 2, 25, 128, 1, 0.5, tie_weights=False)\"\n",
    "    \n",
    "elif args.dataset_name=='MNIST':\n",
    "    from mnist_dataset import *\n",
    "    args.num_classes = 10\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='CIFAR-10':\n",
    "    from cifar10_dataset import *\n",
    "    model_name = \"vgg13()\"\n",
    "    \n",
    "else:\n",
    "    print('Error: The name of the dataset is incorrect. Please re-set the \"dataset_name\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2619, 456, 102, 3037, 1126, 1003, 914, 571, 3016, 419, 2771, 3033, 2233, 356, 2418, 1728, 130, 122, 383, 895]\n"
     ]
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(args, unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = eval(model_name)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.model = worker.model.to(args.device)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.model = worker.model.to('cpu')\n",
    "      del worker.model\n",
    "    self.model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_train,loss_train = train(self.model,args.criterion,self.trainloader,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.model,args.criterion,self.valloader)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.train()\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          running_loss += loss.item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n",
    "  else:\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      model.train()\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.eval()\n",
    "      hidden_test = model.init_hidden(args.test_batch)\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "        if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "          break\n",
    "        data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "        hidden_test = repackage_hidden(hidden_test)\n",
    "        outputs, hidden_test = model(data, hidden_test) \n",
    "        running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "        _, predicted = torch.max(outputs.t(), 1)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss\n",
    "\n",
    "  else:\n",
    "      model.eval()\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        outputs = model(data)\n",
    "        running_loss += criterion(outputs,labels).item()\n",
    "        predicted = torch.argmax(outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('train time：{}[s]'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "server.model.to(args.device)\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  acc_tmp,loss_tmp = test(server.model,args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.local_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.model = copy.deepcopy(server.model)\n",
    "    worker.model = worker.model.to(args.device)\n",
    "    _,_,acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.model = worker.model.to('cpu')\n",
    "    del worker.model\n",
    "\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'FedAvg_{}'.format(args.dataset_name)\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1  loss:4.023088002204896  accuracy:5.688587132616569\n",
      "Epoch2  loss:3.9399458408355716  accuracy:6.273102628509036\n",
      "Epoch3  loss:3.8617472767829897  accuracy:5.689060316980331\n",
      "Epoch4  loss:3.8173423647880553  accuracy:5.153248783572135\n",
      "Epoch5  loss:3.7679908514022817  accuracy:6.3911472822679745\n",
      "Epoch6  loss:3.7530900120735167  accuracy:6.2872323710440945\n",
      "Epoch7  loss:3.724454808235169  accuracy:5.483614799984663\n",
      "Epoch8  loss:3.7168654084205626  accuracy:5.7573027803595735\n",
      "Epoch9  loss:3.7006542921066283  accuracy:6.008320806418245\n",
      "Epoch10  loss:3.6986031651496885  accuracy:5.009779111863261\n",
      "Epoch11  loss:3.6911376833915712  accuracy:5.497220242161534\n",
      "Epoch12  loss:3.680245959758759  accuracy:5.0824603988302615\n",
      "Epoch13  loss:3.6722968339920046  accuracy:5.267497583867447\n",
      "Epoch14  loss:3.669932508468628  accuracy:6.026275836030287\n",
      "Epoch15  loss:3.670416915416718  accuracy:6.299561311944463\n",
      "Epoch16  loss:3.6550448060035707  accuracy:5.393958424614002\n",
      "Epoch17  loss:3.6595420718193057  accuracy:6.986930974397183\n",
      "Epoch18  loss:3.655077707767486  accuracy:7.713458246517161\n",
      "Epoch19  loss:3.6527866601943964  accuracy:6.315497203295637\n",
      "Epoch20  loss:3.639754140377045  accuracy:7.480805685879868\n",
      "Epoch21  loss:3.649943232536316  accuracy:6.500217245158538\n",
      "Epoch22  loss:3.643184530735016  accuracy:6.861153046788477\n",
      "Epoch23  loss:3.6337738037109375  accuracy:6.168099363040654\n",
      "Epoch24  loss:3.6361607193946837  accuracy:6.4405308997579045\n",
      "Epoch25  loss:3.62103157043457  accuracy:6.352706026218747\n",
      "Epoch26  loss:3.6210701227188102  accuracy:6.146562962932826\n",
      "Epoch27  loss:3.6126686930656438  accuracy:6.748911644308865\n",
      "Epoch28  loss:3.61884732246399  accuracy:7.656355142902429\n",
      "Epoch29  loss:3.614686167240143  accuracy:7.006290536946115\n",
      "Epoch30  loss:3.608308637142181  accuracy:8.149161096795538\n",
      "Epoch31  loss:3.6020510435104374  accuracy:6.622460081687088\n",
      "Epoch32  loss:3.602195501327515  accuracy:8.139649249579586\n",
      "Epoch33  loss:3.5883246183395388  accuracy:8.248584155603446\n",
      "Epoch34  loss:3.5867326259613037  accuracy:9.26206276045001\n",
      "Epoch35  loss:3.5806303501129153  accuracy:7.4918352185394275\n",
      "Epoch36  loss:3.5681551337242126  accuracy:8.337231464554803\n",
      "Epoch37  loss:3.570985090732575  accuracy:7.727857088940235\n",
      "Epoch38  loss:3.5534529685974126  accuracy:8.483156269104176\n",
      "Epoch39  loss:3.553774917125702  accuracy:9.799451032458288\n",
      "Epoch40  loss:3.5416119098663335  accuracy:9.262949438065801\n",
      "Epoch41  loss:3.532447791099549  accuracy:9.267849096913197\n",
      "Epoch42  loss:3.530317676067352  accuracy:9.270634534124758\n",
      "Epoch43  loss:3.5256598472595218  accuracy:9.464068611257355\n",
      "Epoch44  loss:3.501285922527314  accuracy:10.80156603831561\n",
      "Epoch45  loss:3.505069315433502  accuracy:11.715866668020567\n",
      "Epoch46  loss:3.5023766398429874  accuracy:10.172244077065127\n",
      "Epoch47  loss:3.475430822372437  accuracy:12.242821543213614\n",
      "Epoch48  loss:3.473787546157836  accuracy:10.876590500794922\n",
      "Epoch49  loss:3.4537518501281737  accuracy:13.639264090424955\n",
      "Epoch50  loss:3.4393581032752993  accuracy:13.321421630387563\n",
      "Epoch51  loss:3.427321541309356  accuracy:13.843190131910161\n",
      "Epoch52  loss:3.417853951454162  accuracy:16.513338800897735\n",
      "Epoch53  loss:3.393083727359771  accuracy:15.030555097903767\n",
      "Epoch54  loss:3.3661377787590028  accuracy:16.491842418514068\n",
      "Epoch55  loss:3.349648034572602  accuracy:16.70997273429351\n",
      "Epoch56  loss:3.3305046916007996  accuracy:19.619469097895895\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/__init__.py\", line 811, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
      "    from .linear import Identity, Linear, Bilinear, LazyLinear\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 7, in <module>\n",
      "    from .. import functional as F\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\", line 18, in <module>\n",
      "    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/_jit_internal.py\", line 29, in <module>\n",
      "    import torch.package._mangling as package_mangling\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/package/__init__.py\", line 12, in <module>\n",
      "    from .package_importer import PackageImporter\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 982, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 925, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1423, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1395, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1550, in find_spec\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/__init__.py\", line 656, in <module>\n",
      "    from .storage import _StorageBase, _TypedStorage, _LegacyStorage, _UntypedStorage\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/storage.py\", line 11, in <module>\n",
      "    import numpy as np\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/numpy/__init__.py\", line 337, in <module>\n",
      "    __mkl_version__ = \"{MajorVersion}.{MinorVersion}.{UpdateVersion}\".format(**mkl.get_version())\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/fp/4z_z8y0j46b1_mp0xj66bjhh0000gn/T/ipykernel_17560/2938473437.py\", line 21, in <cell line: 12>\n",
      "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.local_train()\n",
      "  File \"/var/folders/fp/4z_z8y0j46b1_mp0xj66bjhh0000gn/T/ipykernel_17560/2116892069.py\", line 12, in local_train\n",
      "    acc_train,loss_train = train(self.model,args.criterion,self.trainloader,args.local_epochs)\n",
      "  File \"/var/folders/fp/4z_z8y0j46b1_mp0xj66bjhh0000gn/T/ipykernel_17560/2619747149.py\", line 35, in train\n",
      "    for (data,labels) in trainloader:\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1359, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1325, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1163, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 262, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 429, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object run_code at 0x7fbc66b382f0, file \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3333>, 140447153619696, 74)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 854, in get_records\n",
      "    return list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 565, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 555, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 520, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 369, in executing\n",
      "    args = find(source=cls.for_frame(frame), retry_cache=True)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 252, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 270, in for_filename\n",
      "    result = source_cache[filename] = cls._for_filename_and_lines(filename, lines)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 281, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 79, in __init__\n",
      "    super(Source, self).__init__(*args, **kwargs)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 228, in __init__\n",
      "    self.tree = ast.parse(ast_text, filename=filename)\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "  File \"/Users/jingyaoshi/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 27279) is killed by signal: Interrupt: 2. \n"
     ]
    }
   ],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.local_train()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m acc_valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(acc_valid)\n\u001b[1;32m      4\u001b[0m loss_valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(loss_valid)\n\u001b[0;32m----> 6\u001b[0m acc_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43macc_test\u001b[49m)\n\u001b[1;32m      7\u001b[0m loss_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(loss_test)\n\u001b[1;32m      9\u001b[0m acc_tune_valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(acc_tune_valid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc_test' is not defined"
     ]
    }
   ],
   "source": [
    "acc_train = pd.DataFrame(acc_train)\n",
    "loss_train = pd.DataFrame(loss_train)\n",
    "acc_valid = pd.DataFrame(acc_valid)\n",
    "loss_valid = pd.DataFrame(loss_valid)\n",
    "\n",
    "acc_test = pd.DataFrame(acc_test)\n",
    "loss_test = pd.DataFrame(loss_test)\n",
    "\n",
    "acc_tune_valid = pd.DataFrame(acc_tune_valid)\n",
    "loss_tune_valid = pd.DataFrame(loss_tune_valid)\n",
    "\n",
    "acc_tune_test = pd.DataFrame(acc_tune_test)\n",
    "loss_tune_test = pd.DataFrame(loss_tune_test)\n",
    "\n",
    "\n",
    "acc_train.to_csv(result_path+filename+'_train_acc.csv',index=False, header=False)\n",
    "loss_train.to_csv(result_path+filename+'_train_loss.csv',index=False, header=False)\n",
    "acc_valid.to_csv(result_path+filename+'_valid_acc.csv',index=False, header=False)\n",
    "loss_valid.to_csv(result_path+filename+'_valid_loss.csv',index=False, header=False)\n",
    "acc_test.to_csv(result_path+filename+'_test_acc.csv',index=False, header=False)\n",
    "loss_test.to_csv(result_path+filename+'_test_loss.csv',index=False, header=False)\n",
    "acc_tune_valid.to_csv(result_path+filename+'_fine-tune_valid_acc.csv',index=False, header=False)\n",
    "loss_tune_valid.to_csv(result_path+filename+'_fine-tune_valid_loss.csv',index=False, header=False)\n",
    "acc_tune_test.to_csv(result_path+filename+'_fine-tune_test_acc.csv',index=False, header=False)\n",
    "loss_tune_test.to_csv(result_path+filename+'_fine-tune_test_loss.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
